{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0dfeef-9144-4fa5-aa3f-65fa3f0e3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st  # Importing the Streamlit library and giving it an alias 'st' for building web apps\n",
    "\n",
    "from PyPDF2 import PdfReader  # Importing the PdfReader class from the PyPDF2 library to read PDF files\n",
    "\n",
    "import pandas as pd  # Importing the pandas library and giving it an alias 'pd' for data manipulation and analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Importing the TfidfVectorizer class from scikit-learn for transforming text data into TF-IDF feature vectors\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Importing the cosine_similarity function from scikit-learn for computing the cosine similarity between feature vectors\n",
    "\n",
    "from pptx import Presentation  # Importing the presentation function from\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37930796",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"C:/Users/nrarv/Downloads/project 1 resume_ranking/uplod resume/data-scientist.pdf\"  # Path to the PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8188db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf =PdfReader(file)  # Creating a PdfReader object\n",
    "text = \"\"  # Initializing an empty string to store the text extracted from the PDF file\n",
    "for page in pdf.pages:  # Iterating through each page of the PDF file\n",
    "    text += page.extract_text()  # Extracting text from the current page and appending it to the 'text' string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04788eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n",
      "ROBERT SMITHPhone: (123) 456 78 99 \n",
      "Email: info@qwikresume.com\n",
      "Website: www.qwikresume.com\n",
      "LinkedIn:\n",
      "linkedin.com/qwikresume\n",
      "Address: 1737 Marshville Road,\n",
      "Alabama.\n",
      "Objective\n",
      "Data Scientist with PhD in Physics and 1+ industrial experience. T wo years of working experience \n",
      "in Data Analysis team of LIGO Scientific Collaboration [$3M Special Breakthrough Prize winner of \n",
      "2016]. Over ten years of successful research experience in both theoretical and computational \n",
      "physics. Strong problem-solving and analytical skills. Advanced programming proficiency. Certified\n",
      "in Data Analysis and Machine Learning.\n",
      "Skills\n",
      "Data Mining, Data Analysis, Machine Learning, Python, R, MATLAB, Sphinx, LaT eX, Mathematica, \n",
      "Maple, GIT, CVS, HTCondor.\n",
      "Work Experience\n",
      "Data Scientist\n",
      "ABC Corporation  ­ May 1994 – May 2005  \n",
      "Assisted in determining client needs, deliverable design, estimates and feasibility for \n",
      "analytical projects concerning a custom study for a manufacturer who is using the results to \n",
      "support a litigation claim.\n",
      "Served as an internal resource for Jacknife programming and documentation.\n",
      "Designed and developed small scale deliverables related to the custom study.\n",
      "Participated in the Post Project Review QIP team.\n",
      "Responsible for results reporting in the appropriate media and creation of supporting \n",
      "documentation for the client.\n",
      "Monitored products from statistical programs for accuracy, consistency and statistical validity.\n",
      "Designed and applied statistical and mathematical methods for corporate analytics that were \n",
      "implemented into client-facing products.\n",
      "Data Scientist\n",
      "ABC Corporation  ­ 1993 – 1994 \n",
      "Maintained automated ETL for reporting.\n",
      "Implemented Data mining and machine learning algorithms to describe and predict user \n",
      "behavior on various retailer websites.\n",
      "I revamped their &quot;Predictive Marketing&quot; process to be more data driven and \n",
      "profitable.\n",
      "The new process was able to hone in on more useful user segments that had a significant \n",
      "increase in conversion.\n",
      "Skills Used Data Cleansing and Data Analysis using Python, Scala, R and Spark.\n",
      "Cloud computing on AWS.\n",
      "Automation of reporting..\n",
      "Education\n",
      "© This Free Resume Template  is the copyright of Qwikresume.com . Usage\n",
      "GuidelinesBachelor Of Science - (Stanford)\n",
      "© This Free Resume Template  is the copyright of Qwikresume.com . Usage\n",
      "Guidelines\n"
     ]
    }
   ],
   "source": [
    "print(text)  # Displaying the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17209860",
   "metadata": {},
   "source": [
    "Cleaning Data:\n",
    "\n",
    "1 URLs,\n",
    "2 hashtags,\n",
    "3 mentions,\n",
    "4 special letters,\n",
    "5 punctuations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb089cca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaac88-5c56-4971-bb3f-59c4bbb60c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file): # Define a function that takes the file path as input\n",
    "    pdf = PdfReader(file)  # Create a PdfReader object\n",
    "    text = \"\" # Initialize an empty string to store the extracted text\n",
    "    for page in pdf.pages: # Iterate through each page of the PDF file\n",
    "        text += page.extract_text() # Extract text from the current page and append it to the 'text' string\n",
    "    return text # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346adba-c7be-43e0-8962-c8ea95db7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rank resumes based on job description\n",
    "def rank_resumes(job_description, resumes): # Define a function that takes the job description and resumes as input\n",
    "    # Combine job description with resumes with resumes\n",
    "    \n",
    "    documents = [job_description] + resumes # Combine the job description with resumes\n",
    "    vectorizer = TfidfVectorizer().fit_transform(documents) # Create a TF-IDF Vectorizer object and transform the documents into feature vectors\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    job_description_vector = vectors[0] # Get the feature vector of the job description\n",
    "    \n",
    "    # Get the feature vectors of the resumes\n",
    "    resume_vectors = vectors [1:]\n",
    "    \n",
    "    \n",
    "    cosine_similarities = cosine_similarity ([job_description_vector], resume_vectors).flatten() \n",
    "    # Compute the cosine similarity between the job description and resumes\n",
    "    \n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f053edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit app\n",
    "st.title(\"AI Resume Screening & Candidate Ranking System\")\n",
    "# Job description input\n",
    "st.header(\"Job Description\")\n",
    "job_description= st.text_area (\"Enter the job description\") # Text area for entering the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4202d6-4e09-4119-b1fd-9adb991b4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File uploader\n",
    "st.header(\"Upload Resumes\")\n",
    "uploaded_files = st.file_uploader(\"Upload PDF and PPTX files\", type=[\"pdf\", \"pptx\"], accept_multiple_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e218fd3-19a8-4514-98bb-e5d6672df465",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uploaded_files and job_description: # If files are uploaded and job description is provided\n",
    "    st.header(\"Ranking Resumes\") # Display the 'Ranking Resumes' section header\n",
    "    \n",
    "    resumes = []\n",
    "    for file in uploaded_files:\n",
    "        text = extract_text_from_pdf(file) # Extract text from the uploaded PDF file\n",
    "        resumes.append(text)\n",
    "\n",
    "    # Rank resumes\n",
    "    scores = rank_resumes(job_description, resumes)\n",
    "\n",
    "    # Display scores\n",
    "    results = pd.DataFrame({\"Resume\": [file.name for file in uploaded_files], \"Score\": score }) \n",
    "    # Create a DataFrame with the resume names and scores\n",
    "    \n",
    "    results = results.sort_values(by=\"Score\", ascending=False) \n",
    "    # Sort the results by score in descending order\n",
    "    \n",
    "    \n",
    "    st.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4c751-0615-4257-9dd4-36253f1e31e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
